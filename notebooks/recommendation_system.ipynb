{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f08fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 10:28:07.419031: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-29 10:28:07.653853: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-29 10:28:07.854616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759123688.053584    6459 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759123688.103908    6459 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759123688.496866    6459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759123688.496903    6459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759123688.496905    6459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759123688.496907    6459 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-29 10:28:08.537255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# ML/DL libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a940f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the recommendation system\"\"\"\n",
    "    # Model architecture\n",
    "    embedding_dim: int = 128\n",
    "    user_tower_dims: List[int] = None\n",
    "    item_tower_dims: List[int] = None\n",
    "    cross_layers: int = 3\n",
    "    dnn_dims: List[int] = None\n",
    "    dropout_rate: float = 0.2\n",
    "    l2_reg: float = 1e-5\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 8192\n",
    "    learning_rate_retrieval: float = 0.1\n",
    "    learning_rate_ranking: float = 0.001\n",
    "    epochs_retrieval: int = 10\n",
    "    epochs_ranking: int = 15\n",
    "    warmup_steps: int = 1000\n",
    "    \n",
    "    # Negative sampling\n",
    "    num_hard_negatives: int = 5\n",
    "    num_random_negatives: int = 100\n",
    "    negative_sampling_strategy: str = \"mixed\"  # \"random\", \"hard\", \"mixed\"\n",
    "    \n",
    "    # Multi-task learning\n",
    "    ctr_weight: float = 0.5\n",
    "    rating_weight: float = 0.5\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_topk: List[int] = None\n",
    "    \n",
    "    # System\n",
    "    mixed_precision: bool = True\n",
    "    distributed_strategy: str = \"mirrored\"  # \"mirrored\", \"multi_worker\", \"tpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.user_tower_dims is None:\n",
    "            self.user_tower_dims = [512, 256, 128]\n",
    "        if self.item_tower_dims is None:\n",
    "            self.item_tower_dims = [512, 256, 128]\n",
    "        if self.dnn_dims is None:\n",
    "            self.dnn_dims = [512, 256, 128, 64]\n",
    "        if self.eval_topk is None:\n",
    "            self.eval_topk = [1, 5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a12992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.user_encoder = None\n",
    "        self.item_encoder = None\n",
    "        self.feature_scalers = {}\n",
    "\n",
    "    def load_and_validate_data(self, pickle_path: str) -> Dict[str, any]:\n",
    "        logger.info(f\"Loading data from {pickle_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        except:\n",
    "            try:\n",
    "                data = pd.read_pickle(pickle_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data from {pickle_path}: {e}\")\n",
    "                raise\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            train_df = data.get('train_ratings', data.get('train', pd.DataFrame()))\n",
    "            val_df = data.get('val_ratings', data.get('val', pd.DataFrame()))\n",
    "            test_df = data.get('test_ratings', data.get('test', pd.DataFrame()))\n",
    "            user_features = data.get('user_features', pd.DataFrame())\n",
    "            item_features = data.get('movie_features', pd.DataFrame())\n",
    "        else:\n",
    "            train_df = data\n",
    "            val_df = pd.DataFrame()\n",
    "            test_df = pd.DataFrame()\n",
    "            user_features = pd.DataFrame()\n",
    "            item_features = pd.DataFrame()\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'val_df': val_df,\n",
    "            'test_df': test_df,\n",
    "            'user_features': user_features,\n",
    "            'item_features': item_features,\n",
    "        }\n",
    "\n",
    "    def engineer_features(self, df: pd.DataFrame, user_features: pd.DataFrame, \n",
    "                         item_features: pd.DataFrame, mode: str = 'train') -> pd.DataFrame:\n",
    "        \"\"\"Advanced Feature Engineering\"\"\"\n",
    "        logger.info(f\"Engineering features for {mode} data\")\n",
    "\n",
    "        #Ensure required columns exist\n",
    "        required_cols = ['user_id', 'movie_id']\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                alt_names = {\n",
    "                    'user_id': ['user', 'userId', 'user_idx'],\n",
    "                    'movie_id': ['movie', 'movieId', 'movie_idx', 'item_id'],\n",
    "                }\n",
    "                found = False\n",
    "                for alt_name in alt_names.get(col, []):\n",
    "                    if alt_name in df.columns:\n",
    "                        df = df.rename(columns={alt_name: col})\n",
    "                        found = True\n",
    "                        break\n",
    "                    if not found:\n",
    "                        raise ValueError(f\"Required column {col} not found in dataframe\")\n",
    "        \n",
    "        df['user_id'] = df['user_id'].astype(str)\n",
    "        df['movie_id'] = df['movie_id'].astype(str)\n",
    "\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['hour'] = df['timestamp'].dt.hour\n",
    "            df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "            df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "        #User behavioral features\n",
    "        user_stats = df.groupby('user_id').agg({\n",
    "            'rating': ['count', 'mean', 'std'] if 'rating' in df.columns else ['count'],\n",
    "            'movie_id': 'nunique'\n",
    "        }).fillna(0)\n",
    "\n",
    "        user_stats.columns = ['user_rating_count', 'user_avg_rating', 'user_rating_std', 'user_unique_items']\n",
    "        if 'rating' not in df.columns:\n",
    "            user_stats = user_stats[['user_rating_count', 'user_unique_items']]\n",
    "\n",
    "        #Item popularity features\n",
    "        item_stats = df.groupby('movie_id').agg({\n",
    "            'rating': ['count', 'mean', 'std'] if 'rating' in df.columns else ['count'],\n",
    "            'user_id': 'nunique'\n",
    "        }).fillna(0)\n",
    "\n",
    "        item_stats.columns = ['movie_rating_count', 'movie_avg_rating', 'movie_rating_std', 'movie_unique_users']\n",
    "        if 'rating' not in df.columns:\n",
    "            item_stats = item_stats[['movie_rating_count', 'movie_unique_users']]\n",
    "        \n",
    "        #Merge features\n",
    "        df = df.merge(user_stats, left_on='user_id', right_index = True, how='left')\n",
    "        df = df.merge(item_stats, left_on='movie_id', right_index=True, how='left')\n",
    "\n",
    "        #Merge external features\n",
    "        if not user_features.empty:\n",
    "            user_key = user_features.columns[0]\n",
    "            df = df.merge(user_features, left_on='user_id', right_on=user_key, how='left', suffixes=('', '_user'))\n",
    "        \n",
    "        if not item_features.empty:\n",
    "            item_key = item_features.columns[0]\n",
    "            df = df.merge(user_features, left_on='movie_id', right_on=item_key, how='left', suffixex=('', '_item'))\n",
    "        \n",
    "        #Scale Numerical Features\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        id_cols = ['user_id', 'movie_id', 'rating', 'y_implicit', 'timestamp']\n",
    "        numeric_cols = [col for col in numeric_cols if col not in id_cols]\n",
    "\n",
    "        if numeric_cols:\n",
    "            if mode == 'train':\n",
    "                self.feature_scalers['numeric'] = StandardScaler()\n",
    "                df[numeric_cols] = self.feature_scalers['numeric'].fit_transform(df[numeric_cols])\n",
    "            else:\n",
    "                if 'numeric' in self.feature_scalers:\n",
    "                    df[numeric_cols] = self.feature_scalers['numeric'].transform(df[numeric_cols])\n",
    "            \n",
    "        return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de5ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \"\"\"Advanced negative sampling strategies\"\"\"\n",
    "\n",
    "    def __init__(self, strategy: str='mixed', num_hard: int=5, num_random:int=100):\n",
    "        self.strategy = strategy\n",
    "        self.num_hard = num_hard\n",
    "        self.num_random = num_random\n",
    "        self.item_popularity = None\n",
    "        self.user_item_matrix = None\n",
    "    \n",
    "    def fit(self, train_df:pd.DataFrame):\n",
    "        \"\"\"Fit the negative sampler on training data\"\"\"\n",
    "        logger.info(\"Fitting negative sampler...\")\n",
    "\n",
    "        self.item_popularity = train_df.groupby('movie_id').size().to_dict()\n",
    "        user_items = train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
    "        self.user_item_matrix = user_items\n",
    "\n",
    "        self.all_items = set(train_df['movie_id'].unique())\n",
    "        self.all_items = set(train_df['movie_id'].unique())\n",
    "    \n",
    "    def sample_negatives(self, user_id: str, positive_items: List[str], \n",
    "        item_embeddings: Optional[np.ndarray] = None) -> List[str]:\n",
    "        \"\"\"Sample negative items for a user\"\"\"\n",
    "        if user_id not in self.user_item_matrix:\n",
    "            return list(np.random.choice(list(self.all_items),\n",
    "            size=min(self.num_random, len(self.all_items)), replace=False))\n",
    "        \n",
    "        #Get candidate negatives\n",
    "        user_positive_items = self.user_item_matrix[user_id]\n",
    "        candidate_negatives = self.all_items - user_positive_items\n",
    "\n",
    "        if len(candidate_negatives) == 0:\n",
    "            return []\n",
    "        \n",
    "        negatives = []\n",
    "        if self.strategy in ['random', 'mixed']:\n",
    "            #Random sampling\n",
    "            n_random = self.num_random if self.strategy == 'random' else self.num_random // 2\n",
    "            random_negs = np.random.choice(list(candidate_negatives), size=min(n_random, len(candidate_negatives)), replace=False)\n",
    "            negatives.extend(random_negs)\n",
    "        \n",
    "        if self.strategy in ['hard', 'mixed']:\n",
    "            #Hard sampling\n",
    "            n_hard = self.num_hard if self.strategy == 'hard' else self.num_hard // 2\n",
    "            popularity_scores = [(item, self.item_popularity.get(item, 0)) for item in candidate_negatives]\n",
    "            popularity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            hard_negs = [item for item, _ in popularity_scores[:n_hard]]\n",
    "            negatives.extend(hard_negs)\n",
    "        \n",
    "        return negatives\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c3cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMetrics:\n",
    "    \"\"\"Comprehensive evaluation metrics for recommendation systems\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevance_scores: List[float], k: int) -> float:\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "        dcg = 0\n",
    "        for i, rel in enumerate(relevance_scores):\n",
    "            dcg += (2**rel-1)/np.log2(i+2)\n",
    "        \n",
    "        return dcg\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(true_relevance: List[float], pred_relevance: List[float], k: int) -> float:\n",
    "        \"\"\"Normalized Discounted Cumulative Gain at k\"\"\"\n",
    "\n",
    "        dcg = AdvancedMetrics.dcg_at_k(pred_relevance, k)\n",
    "        idcg = AdvancedMetrics.dcg_at_k(sorted(true_relevance, reverse=True), k)\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def map_at_k(y_true: List[List[str]], y_pred: List[List[str]], k: int) -> float:\n",
    "        \"\"\"Mean Average Precision at k\"\"\"\n",
    "        average_precisions = []\n",
    "        for true_items, pred_items in zip(y_true, y_pred):\n",
    "            true_set = set(true_items)\n",
    "            pred_k = pred_items[:k]\n",
    "\n",
    "            if not true_set:\n",
    "                continue\n",
    "\n",
    "            precisions = []\n",
    "            relevant_count = 0\n",
    "            for i, item in enumerate(pred_k):\n",
    "                if item in true_set:\n",
    "                    relevant_count += 1\n",
    "                    precisions.append(relevant_count/(i+1))\n",
    "            \n",
    "            if precisions:\n",
    "                average_precisions.append(sum(precisions)/len(precisions))\n",
    "            else:\n",
    "                average_precisions.append(0.0)\n",
    "            \n",
    "        return np.mean(average_precisions) if average_precisions else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mrr(y_true: List[List[str]], y_pred: List[List[str]]) -> float:\n",
    "        \"\"\"Mean Reciprocal Rank\"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        for true_items, pred_items in zip(y_true, y_pred):\n",
    "            true_set = set(true_items)\n",
    "            for i, item in enumerate(pred_items, 1):\n",
    "                if item in true_set:\n",
    "                    reciprocal_ranks.append(1/i)\n",
    "                    break\n",
    "            else:\n",
    "                reciprocal_ranks.append(0.0)\n",
    "            \n",
    "        return np.mean(reciprocal_ranks)\n",
    "    \n",
    "    @staticmethod\n",
    "    def coverage(recommendations: List[List[str]], all_items: List[str]) -> float:\n",
    "        \"\"\"Item Coverage - fraction of items that appear in recommendations\"\"\"\n",
    "\n",
    "        recommended_items = set()\n",
    "        for rec_list in recommendations:\n",
    "            recommended_items.update(rec_list)\n",
    "        \n",
    "        return len(recommended_items)/len(all_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def diversity(recommendations: List[List[str]], item_features: Optional[pd.DataFrame] = None) -> float:\n",
    "        \"\"\"Intra-list diversity (average pairwise distance within recommendation lists)\"\"\"\n",
    "        if item_features is None:\n",
    "            #Simple diversity based on unique items\n",
    "            diversities = []\n",
    "            for rec_list in recommendations:\n",
    "                if len(rec_list) <= 1:\n",
    "                    diversities.append(0.0)\n",
    "                else:\n",
    "                    unique_items = len(set(rec_list))\n",
    "                    diversities.append(unique_items/len(rec_list))\n",
    "            \n",
    "            return np.mean(diversities)\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCrossNetwork(keras.Model):\n",
    "    \"\"\"Deep and Cross Network for feature interactions\"\"\"\n",
    "\n",
    "    def __init__(self, cross_layers: int=3, deep_layers: List[int] = [512, 256, 128], dropout_rate: float = 0.2,\n",
    "        l2_reg: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.cross_layers = cross_layers\n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        #Cross Layers\n",
    "        self.cross_weights = []\n",
    "        self.cross_biases = []\n",
    "\n",
    "        #Deep layers\n",
    "        self.deep_nets = []\n",
    "        for units in deep_layers:\n",
    "            self.deep_nets.append(tf.keras.layers.Dense(\n",
    "                units, activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "            ))\n",
    "            self.deep_nets.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        #Initialize cross layer weights\n",
    "        for i in range(self.cross_layers):\n",
    "            self.cross_weights.append(\n",
    "                self.add_weight(\n",
    "                    name = f'cross_weight_{i}',\n",
    "                    shape = (input_dim, 1),\n",
    "                    initializer = 'truncated_normal',\n",
    "                    regularizer = tf.keras.regularizers.l2(self.l2_reg)\n",
    "                )\n",
    "            )\n",
    "            self.cross_biases.append(\n",
    "                self.add_weight(\n",
    "                    name=f'cross_bias_{i}',\n",
    "                    shape = (input_dim,),\n",
    "                    initializer = 'zeros'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        #Cross Network\n",
    "        x0 = inputs\n",
    "        xl = x0\n",
    "\n",
    "        for i in range(self.cross_layers):\n",
    "            xl_w = tf.matmul(xl, self.cross_weights)\n",
    "            xl = x0 * xl_w + self.cross_biases + xl\n",
    "            \n",
    "        cross_output = xl\n",
    "\n",
    "        # Deep Network\n",
    "        deep_inputs = inputs\n",
    "        for layer in self.deep_nets:\n",
    "            deep_output = layer(deep_output, training=training)\n",
    "\n",
    "        combined = tf.concat([cross_output, deep_output], axis=1)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540ad062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTowerModel(keras.Model):\n",
    "    \"\"\"Multi-tower model architecture for retrieval\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig, user_vocab: List[str],\n",
    "        item_vocab: List[str], feature_specs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.user_vocab = user_vocab\n",
    "        self.item_vocab = item_vocab\n",
    "        self.feature_specs = feature_specs\n",
    "\n",
    "        #User Tower\n",
    "        self.user_lookup = keras.layers.StringLookup(vocabulary=user_vocab,\n",
    "                mask_token=None)\n",
    "        self.user_embedding = keras.layers.Embedding(\n",
    "            len(user_vocab)+1, config.embedding_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(config.l2_reg)\n",
    "        )\n",
    "        \n",
    "        #Item Tower\n",
    "        self.item_lookup = keras.layers.StringLookup(vocabulary=item_vocab,\n",
    "                mask_token=None)\n",
    "        self.item_embedding = keras.layers.Embedding(\n",
    "            len(item_vocab)+1, config.embedding_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(config.l2_reg)\n",
    "        )\n",
    "\n",
    "        #Feature Processing Layers\n",
    "        self.feature_layers = {}\n",
    "        for feature_name, feature_info in feature_specs.items():\n",
    "            if feature_info['type'] == 'categorical':\n",
    "                self.feature_layers[feature_name] = keras.layers.Embedding(\n",
    "                    feature_info['vocab_size'], config.embedding_dim // 2\n",
    "                )\n",
    "            elif feature_info['type'] == 'numerical':\n",
    "                self.feature_layers[feature_name] = keras.layers.Dense(\n",
    "                    config.embedding_dim // 2, activation='relu'\n",
    "                )\n",
    "        \n",
    "        # Tower Networks\n",
    "        self.user_tower_layers = []\n",
    "        for units in config.user_tower_dims:\n",
    "            self.user_tower_layers.extend([\n",
    "                keras.layers.Dense(units, activation='relu'),\n",
    "                keras.layers.Dropout(config.dropout_rate)\n",
    "            ])\n",
    "        self.user_tower_layers.append(keras.layers.Dense(config.embedding_dim))\n",
    "\n",
    "        self.item_tower_layers = []\n",
    "        for units in config.item_tower_dims:\n",
    "            self.item_tower_layers.extend([\n",
    "                keras.layers.Dense(units, activation='relu'),\n",
    "                keras.layers.Dropout(config.dropout_rate)\n",
    "            ])\n",
    "        self.item_tower_layers.append(keras.layers.Dense(config.embedding_dim))\n",
    "\n",
    "        def call(self, features, training=None):\n",
    "            # User Tower\n",
    "            user_emb = self.user_embedding(self.user_lookup(features['user_id']))\n",
    "            user_emb = tf.squeeze(user_emb, axis=1)\n",
    "\n",
    "            # Process user features\n",
    "            user_feature_embs = [user_emb]\n",
    "            for feature_name, layer in self.feature_layers.items():\n",
    "                if(feature_name.startswith('user_') and feature_name in features):\n",
    "                    feat_emb = layer(features[feature_name])\n",
    "                    if len(feat_emb.shape) > 2:\n",
    "                        feat_emb = tf.squeeze(feat_emb, axis=1)\n",
    "                    user_feature_embs.append(feat_emb)\n",
    "            \n",
    "            user_combined = tf.concat(user_feature_embs, axis=1)\n",
    "\n",
    "            for layer in self.user_tower_layers:\n",
    "                user_combined = layer(user_combined, training=training)\n",
    "\n",
    "            # Item tower\n",
    "            item_emb = self.item_embedding(self.item_lookup(features['movie_id']))\n",
    "            item_emb = tf.squeeze(item_emb, axis=1)\n",
    "            \n",
    "            # Process item features\n",
    "            item_feature_embs = [item_emb]\n",
    "            for feature_name, layer in self.feature_layers.items():\n",
    "                if feature_name.startswith('movie_') and feature_name in features:\n",
    "                    feat_emb = layer(features[feature_name])\n",
    "                    if len(feat_emb.shape) > 2:\n",
    "                        feat_emb = tf.squeeze(feat_emb, axis=1)\n",
    "                    item_feature_embs.append(feat_emb)\n",
    "            \n",
    "            item_combined = tf.concat(item_feature_embs, axis=1)\n",
    "            \n",
    "            for layer in self.item_tower_layers:\n",
    "                item_combined = layer(item_combined, training=training)\n",
    "            \n",
    "            return {\n",
    "                'user_embedding': user_combined,\n",
    "                'item_embedding': item_combined\n",
    "            }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(tfrs.models.Model):\n",
    "    def __init__(self, config: ModelConfig, user_vocab: List[str],\n",
    "        item_vocab: List[str], feature_specs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = MultiTowerModel(config, user_vocab, item_vocab, feature_specs)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
