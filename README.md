# ğŸ¬ Enterprise-Scale Recommendation System

A production-ready recommendation system using **two-tower retrieval** and **Deep & Cross Network (DCN)** ranking models. Built following industry best practices from systems like YouTube, Netflix, and TikTok.

---

## ğŸš€ Features

### Two-Stage Architecture
- **Retrieval Stage**: Multi-tower embeddings with dot-product similarity
- **Ranking Stage**: Deep & Cross Network (DCN) for feature interactions

### Advanced Capabilities
- âœ… Multiple negative sampling strategies (Random, Hard, Mixed)
- âœ… Rich feature engineering (user/item statistics, temporal features)
- âœ… Comprehensive evaluation metrics (Recall@K, Precision@K, NDCG, MAP, MRR)
- âœ… FAISS-based approximate nearest neighbor search
- âœ… Multi-task learning (CTR + Rating prediction)
- âœ… Class weights for handling imbalanced datasets
- âœ… FastAPI REST API with Docker support
- âœ… Modular, production-ready codebase
- âœ… W&B integration for experiment tracking

---

## ğŸ“‚ Project Structure

```
Recommendation System/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package setup
â”œâ”€â”€ QUICK_START.md              # Quick start guide
â”‚
â”œâ”€â”€ data/                        # Data directory
â”‚   â”œâ”€â”€ raw/                     # Raw MovieLens dataset
â”‚   â”‚   â”œâ”€â”€ movies.dat
â”‚   â”‚   â”œâ”€â”€ ratings.dat
â”‚   â”‚   â”œâ”€â”€ users.dat
â”‚   â”‚   â””â”€â”€ README
â”‚   â””â”€â”€ processed/               # Preprocessed data (gitignored)
â”‚       â””â”€â”€ processed_data.pkl
â”‚
â”œâ”€â”€ src/                         # Source code package
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ config.py                # Model configuration
â”‚   â”œâ”€â”€ models.py                # Model architectures (DCN, MultiTower)
â”‚   â”œâ”€â”€ data_processing.py       # Data loading and feature engineering
â”‚   â”œâ”€â”€ evaluation.py            # Metrics and callbacks
â”‚   â”œâ”€â”€ trainer.py               # Training orchestration
â”‚   â””â”€â”€ preprocessing.py         # Dataset preprocessing pipeline
â”‚
â”œâ”€â”€ app/                         # FastAPI application
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ main.py                  # FastAPI main application
â”‚   â”œâ”€â”€ recommendation_service.py # Recommendation service
â”‚   â”œâ”€â”€ simple_model_loader.py   # Simple model loader
â”‚   â”œâ”€â”€ model_service.py         # Model service
â”‚   â”œâ”€â”€ start_api.sh             # API startup script
â”‚   â”œâ”€â”€ test_api.py              # API testing script
â”‚   â”œâ”€â”€ docker-compose.yml       # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile               # Docker image
â”‚   â””â”€â”€ requirements.txt         # API dependencies
â”‚
â”œâ”€â”€ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ train.py                 # Training entrypoint
â”‚   â””â”€â”€ preprocess.py            # Data preprocessing script
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 01_exploratory_data_analysis.ipynb
â”‚   â””â”€â”€ 02_basic_analysis.ipynb
â”‚
â”œâ”€â”€ outputs/                     # Training outputs (gitignored)
â”‚   â”œâ”€â”€ models/                  # Saved models
â”‚   â”‚   â””â”€â”€ experiment_001/      # Experiment outputs
â”‚   â”œâ”€â”€ logs/                    # Training logs
â”‚   â””â”€â”€ metrics/                 # Evaluation results
â”‚
â””â”€â”€ configs/                     # Configuration files
```

---

## ğŸ—‚ Dataset

This project uses the **MovieLens** dataset (1M ratings). The dataset includes:
- User ratings for movies
- Movie metadata (title, genres)
- User demographics (age, gender, occupation)

### Data Splits
- **Training**: 80% (temporal)
- **Validation**: 10% (temporal)
- **Testing**: 10% (temporal)

---

## âš™ï¸ Installation

### 1. Clone the Repository
```bash
git clone <repository-url>
cd recommendation-system
```

### 2. Install Dependencies
```bash
# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 3. (Optional) Install GPU Support
```bash
# For FAISS GPU support
pip install faiss-gpu

# Verify TensorFlow GPU
python -c "import tensorflow as tf; print('GPU Available:', len(tf.config.list_physical_devices('GPU')) > 0)"
```

---

## ğŸš€ Quick Start

### Step 1: Preprocess the Data

```bash
python scripts/preprocess.py \
    --data_dir data/raw \
    --output data/processed/processed_data.pkl
```

### Step 2: Train the Model

```bash
python scripts/train.py \
    --data data/processed/processed_data.pkl \
    --output_dir outputs/models/experiment_001 \
    --embedding_dim 128 \
    --batch_size 4096 \
    --epochs 5 \
    --learning_rate 0.01 \
    --negative_sampling mixed
```

### Step 3: Start the API Server

```bash
cd app
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### Step 4: Test the API

```bash
# Health check
curl http://localhost:8000/health

# Get recommendations for a user
curl -X POST "http://localhost:8000/recommendations" \
     -H "Content-Type: application/json" \
     -d '{"user_id": 1001, "num_recommendations": 10}'
```

### Step 5: View Results

Training artifacts will be saved to `outputs/models/experiment_001/`:
- `best_model.keras` - Best model checkpoint
- `encoder.keras` - Encoder model for inference
- `faiss.idx` - FAISS index for fast retrieval
- `training_log.csv` - Training metrics per epoch
- `metrics.json` - Final evaluation metrics
- `logs/` - TensorBoard logs

View with TensorBoard:
```bash
tensorboard --logdir outputs/models/experiment_001/logs
```

---

## ğŸŒ API Documentation

### FastAPI Endpoints

The recommendation system includes a FastAPI server with the following endpoints:

#### Health Check
```http
GET /health
```
Returns the health status of the API.

#### Get Recommendations
```http
POST /recommendations
Content-Type: application/json

{
    "user_id": 1,
    "num_recommendations": 10
}
```

**Response:**
```json
{
    "user_id": 1,
    "recommendations": [
        {"item_id": 123, "score": 0.95},
        {"item_id": 456, "score": 0.89}
    ],
    "num_recommendations": 10
}
```

#### Get Similar Items
```http
POST /similar_items
Content-Type: application/json

{
    "item_id": 123,
    "num_similar": 5
}
```

### Running the API

#### Development Mode
```bash
cd app
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

#### Production Mode
```bash
cd app
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

#### Using Docker
```bash
cd app
docker-compose up --build
```

### API Testing

Test the API endpoints:
```bash
# Run the test script
python app/test_api.py

# Or test manually
curl http://localhost:8000/health
```

---

## ğŸ“Š Model Architecture

### Retrieval Model (Two-Tower)
```
User Tower:                    Item Tower:
  User ID Embedding              Item ID Embedding
  + Feature Embeddings           + Feature Embeddings
       â†“                              â†“
  Dense(256) + BN + Dropout      Dense(256) + BN + Dropout
       â†“                              â†“
  Dense(128) + BN + Dropout      Dense(128) + BN + Dropout
       â†“                              â†“
  Dense(128)                     Dense(128)
       â†“                              â†“
  User Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Dot Product Similarity
```

### Ranking Model (DCN)
```
[User Emb || Item Emb]
       â†“
Deep & Cross Network
  â”œâ”€ Cross Layers (3x)
  â””â”€ Deep Layers [256, 128, 64]
       â†“
    Concat
       â†“
  â”œâ”€ Rating Head (MSE)
  â””â”€ CTR Head (BCE)
```

### Multi-Task Loss
```
L_total = L_retrieval + Î± * L_rating + Î² * L_ctr
```

---

## ğŸ“ˆ Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **Recall@K** | Fraction of relevant items retrieved in top-K |
| **Precision@K** | Fraction of top-K items that are relevant |
| **NDCG@K** | Normalized Discounted Cumulative Gain (rank-aware) |
| **MAP@K** | Mean Average Precision across all ranks |
| **MRR** | Mean Reciprocal Rank of first relevant item |
| **Coverage** | Fraction of catalog items recommended |
| **Diversity** | Uniqueness of items in recommendation lists |

---

## ğŸ”§ Configuration

### Key Hyperparameters

Edit `src/config.py` or pass arguments to `scripts/train.py`:

```python
ModelConfig(
    # Architecture
    embedding_dim=128,              # Embedding dimension
    user_tower_dims=[256, 128],     # User tower architecture
    item_tower_dims=[256, 128],     # Item tower architecture
    cross_layers=3,                 # DCN cross layers
    dnn_dims=[256, 128, 64],        # DCN deep layers
    
    # Training
    batch_size=2048,
    learning_rate_retrieval=0.01,
    epochs_retrieval=5,
    
    # Negative Sampling
    negative_sampling_strategy="mixed",  # random, hard, mixed
    num_hard_negatives=5,
    num_random_negatives=50,
    
    # Multi-task weights
    ctr_weight=0.3,
    rating_weight=0.7,
    
    # Class weights for imbalanced datasets
    use_class_weights=True,  # Automatically calculated for balanced training
)
```

---

## ğŸ¯ Advanced Usage

### Custom Training with W&B

```bash
python scripts/train.py \
    --data data/processed/processed_data.pkl \
    --output_dir outputs/models/experiment_001 \
    --use_wandb \
    --embedding_dim 128 \
    --epochs 20
```

### Distributed Training (Multi-GPU)

```bash
python scripts/train.py \
    --data data/processed/processed_data.pkl \
    --output_dir outputs/models/distributed_run \
    --distributed_strategy mirrored \
    --batch_size 8192
```

### Hyperparameter Tuning

```bash
# Example: Test different negative sampling strategies
for strategy in random hard mixed; do
    python scripts/train.py \
        --data data/processed/processed_data.pkl \
        --output_dir outputs/models/neg_sampling_${strategy} \
        --negative_sampling ${strategy}
done
```

---

## ğŸ“ Code Organization

### Source Modules

- **`src/config.py`**: Configuration dataclass with all hyperparameters
- **`src/models.py`**: Model architectures (DeepCrossNetwork, MultiTowerModel, MultiTaskModel)
- **`src/data_processing.py`**: Data loading, feature engineering, negative sampling
- **`src/evaluation.py`**: Evaluation metrics and custom callbacks
- **`src/trainer.py`**: Training orchestration and artifact management
- **`src/preprocessing.py`**: MovieLens data preprocessing pipeline

### Scripts

- **`scripts/train.py`**: Main training entrypoint with CLI arguments

---

## ğŸ› Troubleshooting

### Out of Memory (OOM)
```bash
# Reduce batch size
python scripts/train.py --batch_size 2048 ...

# Use mixed precision (automatic with GPU)
# Already enabled when GPU is detected
```

### Slow Training
```bash
# Increase batch size (if you have memory)
python scripts/train.py --batch_size 8192 ...

# Reduce model complexity
python scripts/train.py --embedding_dim 64 --cross_layers 2 ...
```

### Import Errors
```bash
# Make sure you're in the project root
cd /path/to/recommendation-system

# Install in development mode (optional)
pip install -e .
```

---

## ğŸ›  Future Enhancements

- [x] Add REST API with FastAPI
- [x] Add Docker containerization
- [ ] Implement online learning capabilities
- [ ] Add diversity constraints in ranking
- [ ] Session-based recommendations (RNN/Transformer)
- [ ] A/B testing framework
- [ ] Deployment to Kubernetes
- [ ] Add model versioning and A/B testing
- [ ] Implement real-time feature serving
- [ ] Add monitoring and alerting

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“š References

- [Deep & Cross Network (DCN)](https://arxiv.org/abs/1708.05123)
- [Two-Tower Models for Retrieval](https://research.google/pubs/pub48840/)
- [TensorFlow Recommenders](https://www.tensorflow.org/recommenders)
- [FAISS](https://github.com/facebookresearch/faiss)

---

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.
